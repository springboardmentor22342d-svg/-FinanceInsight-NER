{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joHaBYUhRJZb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 1. Setup tools for cleaning text (Updated with the missing resource)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# --- CHANGE THIS TO YOUR ACTUAL FOLDER PATH ---\n",
        "folder_path = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"The 'Cleaning Machine': Turns messy text into AI-ready text.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = text.replace('$', ' dollar ').replace('‚Ç¨', ' euro ')\n",
        "    text = re.sub(r'[^a-zA-Z0-9.% ]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# 2. LOAD DATASET 1: NER Data (train.txt)\n",
        "try:\n",
        "    with open(os.path.join(folder_path, 'train.txt'), 'r') as f:\n",
        "        ner_lines = f.readlines()\n",
        "    ner_text = \" \".join([line.split()[0] for line in ner_lines if len(line.split()) > 0])\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.txt not found. Check your folder_path.\")\n",
        "    ner_text = \"\"\n",
        "\n",
        "# 3. LOAD DATASET 2: Financial Sentences (Sentences_AllAgree.txt)\n",
        "try:\n",
        "    with open(os.path.join(folder_path, 'Sentences_AllAgree.txt'), 'r', encoding='latin-1') as f:\n",
        "        sentiment_sentences = f.readlines()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Sentences_AllAgree.txt not found.\")\n",
        "    sentiment_sentences = []\n",
        "\n",
        "# 4. LOAD DATASET 3: Tabular Data (demo_facts.csv)\n",
        "try:\n",
        "    facts_df = pd.read_csv(os.path.join(folder_path, 'demo_facts.csv'))\n",
        "    facts_sentences = facts_df.iloc[:, 0].astype(str).tolist()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: demo_facts.csv not found.\")\n",
        "    facts_sentences = []\n",
        "\n",
        "# 5. COMBINE AND CLEAN\n",
        "master_list = [ner_text] + sentiment_sentences + facts_sentences\n",
        "# Filter out empty strings if any files failed to load\n",
        "master_list = [s for s in master_list if s.strip()]\n",
        "\n",
        "cleaned_data = [clean_text(doc) for doc in master_list]\n",
        "\n",
        "# 6. SAVE BACK TO DRIVE\n",
        "final_report = pd.DataFrame({\n",
        "    'Original_Data': master_list,\n",
        "    'AI_Ready_Data': cleaned_data\n",
        "})\n",
        "\n",
        "output_path = os.path.join(folder_path, 'combined_financial_data.csv')\n",
        "final_report.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Success! Your combined file is saved at: {output_path}\")\n",
        "print(f\"Total rows processed: {len(final_report)}\")"
      ],
      "metadata": {
        "id": "Ol0Iztl5d-4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# 1. Load your cleaned data\n",
        "# Make sure this path matches where you saved the file in the previous step\n",
        "file_path = '/content/drive/MyDrive/dataset/combined_financial_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"--- Dataset Overview ---\")\n",
        "print(f\"Total Rows: {len(df)}\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. Analyze Sentence Lengths\n",
        "# We want to know how many words are in each sentence on average.\n",
        "df['word_count'] = df['AI_Ready_Data'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['word_count'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Sentence Lengths (Word Count)')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 3. Analyze Word Frequency (Most Common Financial Terms)\n",
        "# This finds the top 20 most used words in your dataset.\n",
        "all_words = \" \".join(df['AI_Ready_Data'].astype(str)).split()\n",
        "word_freq = Counter(all_words)\n",
        "common_words = word_freq.most_common(20)\n",
        "\n",
        "# Convert to a DataFrame for easy plotting\n",
        "words_df = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Count', y='Word', data=words_df, palette='viridis')\n",
        "plt.title('Top 20 Most Frequent Words in Cleaned Dataset')\n",
        "plt.show()\n",
        "\n",
        "# 4. Save basic stats for your Mentor report\n",
        "with open('/content/drive/MyDrive/dataset/eda_summary.txt', 'w') as f:\n",
        "    f.write(f\"Total rows: {len(df)}\\n\")\n",
        "    f.write(f\"Average word count: {df['word_count'].mean():.2f}\\n\")\n",
        "    f.write(f\"Top 5 words: {common_words[:5]}\\n\")\n",
        "\n",
        "print(\"EDA Complete! Charts generated and eda_summary.txt saved to Drive.\")"
      ],
      "metadata": {
        "id": "_Xf6rKFdgf_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# 1. Load your combined data\n",
        "file_path = '/content/drive/MyDrive/dataset/combined_financial_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Define the \"Rules\" (Valid Labels) from your mentor's file\n",
        "valid_labels = {\n",
        "    \"O\",\n",
        "    \"B-ORG\", \"I-ORG\",\n",
        "    \"B-METRIC\", \"I-METRIC\",\n",
        "    \"B-VALUE\", \"I-VALUE\",\n",
        "    \"B-DATE\", \"I-DATE\",\n",
        "    \"B-EVENT\", \"I-EVENT\",\n",
        "    \"B-TICKER\", \"I-TICKER\"\n",
        "}\n",
        "\n",
        "def validate_row(text):\n",
        "    \"\"\"\n",
        "    Checks if a row of text can be properly tokenized for NER.\n",
        "    Since we don't have human-labeled tags for everything yet,\n",
        "    this checks if the text is clean enough to BE labeled.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return \"Empty Text\"\n",
        "\n",
        "    # In a real labeled dataset, we would check if len(tokens) == len(labels)\n",
        "    # For now, we check if the text is too short or has illegal characters\n",
        "    if len(str(text).split()) < 3:\n",
        "        return \"Text too short (possible noise)\"\n",
        "\n",
        "    return \"Valid\"\n",
        "\n",
        "# 3. Run Validation\n",
        "print(\"--- Starting Data Validation ---\")\n",
        "df['validation_status'] = df['AI_Ready_Data'].apply(validate_row)\n",
        "\n",
        "# 4. Show Results\n",
        "error_counts = df['validation_status'].value_counts()\n",
        "print(\"\\nValidation Report:\")\n",
        "print(error_counts)\n",
        "\n",
        "# 5. Filter out bad data\n",
        "clean_df = df[df['validation_status'] == \"Valid\"].copy()\n",
        "print(f\"\\nOriginal Row Count: {len(df)}\")\n",
        "print(f\"Clean Row Count: {len(clean_df)}\")\n",
        "\n",
        "# Save the final validated version for training\n",
        "clean_df.to_csv('/content/drive/MyDrive/dataset/validated_financial_data.csv', index=False)\n",
        "print(\"\\nSuccess! Validated data saved to 'validated_financial_data.csv'.\")"
      ],
      "metadata": {
        "id": "QSnaqP7XorFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets seqeval"
      ],
      "metadata": {
        "id": "Ohwk4DHzpPJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
        "\n",
        "# 1. SETUP PATHS\n",
        "# Make sure this points to where your train.txt is\n",
        "folder_path = '/content/drive/MyDrive/dataset/'\n",
        "train_path = folder_path + 'train.txt'\n",
        "\n",
        "# 2. FUNCTION TO READ THE DATA (WORDS + LABELS)\n",
        "def read_conll_file(file_path):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sent = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: # Empty line means end of sentence\n",
        "                if current_sent:\n",
        "                    sentences.append(current_sent)\n",
        "                    labels.append(current_labels)\n",
        "                    current_sent = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2: # Ensure we have Word + Label\n",
        "                    current_sent.append(parts[0])\n",
        "                    current_labels.append(parts[-1]) # The last part is the label\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "print(\"Reading train.txt...\")\n",
        "train_tokens, train_labels = read_conll_file(train_path)\n",
        "print(f\"Success! Loaded {len(train_tokens)} sentences with labels.\")\n",
        "\n",
        "# 3. CREATE LABEL MAPS (The AI needs numbers, not text)\n",
        "# We find all unique labels (B-ORG, I-ORG, etc.) and give them IDs\n",
        "unique_labels = sorted(list(set(label for sent in train_labels for label in sent)))\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(f\"Found {len(unique_labels)} unique labels: {unique_labels}\")\n",
        "\n",
        "# 4. PREPARE FOR HUGGING FACE\n",
        "raw_dataset = Dataset.from_dict({\n",
        "    \"id\": list(range(len(train_tokens))),\n",
        "    \"tokens\": train_tokens,\n",
        "    \"ner_tags\": [[label2id[l] for l in sent] for sent in train_labels]\n",
        "})\n",
        "\n",
        "# 5. LOAD FINBERT TOKENIZER\n",
        "# We use 'yiyanghkust/finbert-tone' as a base, or standard bert if preferred.\n",
        "# For this internship, a standard BERT or FinBERT base works.\n",
        "model_checkpoint = \"yiyanghkust/finbert-pretrain\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# 6. THE MENTOR'S FIX (Logic from jj.txt)\n",
        "# This aligns the labels when the tokenizer splits words\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100) # Special ID for ignored tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx]) # Start of a new word\n",
        "            else:\n",
        "                label_ids.append(-100) # Sub-word (e.g., '##ments') gets ignored\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# 7. APPLY THE FIX\n",
        "print(\"Aligning tokens and labels...\")\n",
        "tokenized_datasets = raw_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "print(\"Data Preparation Complete! Ready for Training.\")"
      ],
      "metadata": {
        "id": "PRbRL3kopYDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "\n",
        "# --- RE-DEFINE MODEL NAME (Just in case) ---\n",
        "model_checkpoint = \"yiyanghkust/finbert-pretrain\"\n",
        "\n",
        "# 1. SETUP THE MODEL\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=len(unique_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# 2. SETUP TRAINING SETTINGS (Fixed the Multiple Values Error)\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/dataset/checkpoints\", # This is now the ONLY output dir\n",
        "    eval_strategy = \"no\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# 3. BATCH THE DATA\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# 4. INITIALIZE THE TRAINER\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# 5. START TRAINING\n",
        "print(\"Starting Training... (This may take 10-15 minutes)\")\n",
        "trainer.train()\n",
        "\n",
        "# 6. SAVE YOUR FINAL MODEL\n",
        "save_path = \"/content/drive/MyDrive/dataset/MyFinBERT_Model\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"TRAINING COMPLETE! Your custom model is saved at: {save_path}\")"
      ],
      "metadata": {
        "id": "JAu4s_YOqKr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import os\n",
        "\n",
        "# 1. CREATE THE TEST FILE MANUALLY (To fix the \"File Not Found\" error)\n",
        "test_sentences = [\n",
        "    \"Microsoft posted operating income of $83.4 billion for fiscal year 2023.\",\n",
        "    \"Amazon reported a net profit of $10.6 billion in the second quarter of 2024.\",\n",
        "    \"Tesla recorded vehicle deliveries of 443,956 units during Q1 2023.\",\n",
        "    \"Reliance Industries announced a capital expenditure plan worth ‚Çπ75,000 crore for FY25.\",\n",
        "    \"JPMorgan Chase generated revenue of $39.9 billion in Q4 2023.\",\n",
        "    \"Meta Platforms saw advertising revenue rise to $31.5 billion in the first quarter of 2024.\",\n",
        "    \"Infosys declared an earnings per share of ‚Çπ18.3 for the quarter ended March 2024.\",\n",
        "    \"Alphabet reported free cash flow of $17.1 billion in Q3 2023.\",\n",
        "    \"Tata Motors completed the acquisition of Iveco‚Äôs defense business in December 2023.\",\n",
        "    \"Netflix added 8.8 million subscribers during the third quarter of 2023.\",\n",
        "    \"Goldman Sachs reported return on equity of 7.5% for the full year 2023.\",\n",
        "    \"Samsung Electronics posted semiconductor losses of ‚Ç©4.6 trillion in Q2 2023.\",\n",
        "    \"Boeing recorded commercial airplane deliveries of 528 units in 2023.\",\n",
        "    \"PayPal announced a share buyback program valued at $5 billion in January.\"\n",
        "]\n",
        "\n",
        "# 2. LOAD YOUR SAVED MODEL\n",
        "# Note: Ensure this path is where you actually saved the model in the previous step\n",
        "model_path = \"/content/drive/MyDrive/dataset/MyFinBERT_Model\"\n",
        "\n",
        "try:\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "except OSError:\n",
        "    print(\"\\nCRITICAL ERROR: Model not found!\")\n",
        "    print(f\"Please check if your model is actually saved at: {model_path}\")\n",
        "    print(\"If you saved it in 'FinanceProject' instead of 'dataset', change the path above.\")\n",
        "    raise\n",
        "\n",
        "# 3. RUN PREDICTIONS\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "print(\"\\n--- AI PREDICTION REPORT ---\\n\")\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "    results = ner_pipeline(sentence)\n",
        "\n",
        "    if len(results) > 0:\n",
        "        print(\"  Found Entities:\")\n",
        "        for entity in results:\n",
        "            score = f\"{entity['score']:.2f}\"\n",
        "            print(f\"    -> {entity['word']} ({entity['entity_group']}) [Conf: {score}]\")\n",
        "    else:\n",
        "        print(\"  No entities found.\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "zy1dbjlMwU7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_financial_data(sentence, ner_pipeline):\n",
        "    # 1. AI LAYER: Find Company\n",
        "    ai_results = ner_pipeline(sentence)\n",
        "    company = \"Unknown\"\n",
        "    for entity in ai_results:\n",
        "        if entity['entity_group'] == 'ORG':\n",
        "            company = entity['word']\n",
        "            break\n",
        "\n",
        "    # 2. LOGIC LAYER: Extended Rules for Milestone 3\n",
        "    # Improved patterns to catch non-currency numbers and complex metrics\n",
        "\n",
        "    # Metrics: Added 'subscribers', 'eps', 'return on equity', 'acquisition', 'buyback'\n",
        "    metric_pattern = r'(?:operating income|net profit|revenue|sales|deliveries|expenditure|cash flow|losses|earnings per share|subscribers|return on equity|acquisition|share buyback)'\n",
        "\n",
        "    # Values: Now catches \"8.8 million\", \"7.5%\", and \"‚Çπ18.3\"\n",
        "    # Logic: (Currency OR Number) + (Number) + (Optional Multiplier like billion/%)\n",
        "    value_pattern = r'(?:[\\$‚Ç¨‚Çπ‚Ç©]\\s?|Rs\\.?\\s?)?\\d+(?:,\\d+)*(?:\\.\\d+)?\\s?(?:billion|million|trillion|crore|lakh|%|units)?'\n",
        "\n",
        "    # Dates: Same as before\n",
        "    date_pattern = r'(?:Q[1-4]|fiscal year|FY)\\s?\\d{2,4}|\\b20\\d{2}\\b'\n",
        "\n",
        "    metric_match = re.search(metric_pattern, sentence, re.IGNORECASE)\n",
        "    value_match = re.search(value_pattern, sentence, re.IGNORECASE)\n",
        "    date_match = re.search(date_pattern, sentence, re.IGNORECASE)\n",
        "\n",
        "    # Clean up the output\n",
        "    return {\n",
        "        \"company\": company,\n",
        "        \"metric\": metric_match.group(0) if metric_match else \"N/A\",\n",
        "        \"value\": value_match.group(0) if value_match else \"N/A\",\n",
        "        \"period\": date_match.group(0) if date_match else \"N/A\"\n",
        "    }\n",
        "\n",
        "# --- RUN FINAL CHECK ---\n",
        "print(\"\\n--- PERFECTED FINANCIAL REPORT ---\\n\")\n",
        "results_list = []\n",
        "for sentence in test_sentences:\n",
        "    data = extract_financial_data(sentence, ner_pipeline)\n",
        "    # Only print if we found a company (filters out noise)\n",
        "    if data['company'] != \"Unknown\":\n",
        "        print(data)\n",
        "        results_list.append(data)\n",
        "\n",
        "# Save for Mentor\n",
        "import json\n",
        "with open('/content/drive/MyDrive/dataset/milestone3_final_output.json', 'w') as f:\n",
        "    json.dump(results_list, f, indent=4)"
      ],
      "metadata": {
        "id": "sy7b68KPyXuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "zt2KV658pLno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "pdf_path = '/content/drive/MyDrive/dataset/2023_Annual_Report.pdf'\n",
        "model_path = \"/content/drive/MyDrive/dataset/MyFinBERT_Model\"\n",
        "output_path = '/content/drive/MyDrive/dataset/final_submission.json'\n",
        "\n",
        "# --- 1. LOAD AI MODEL ---\n",
        "print(\"Loading FinBERT Model...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "except Exception:\n",
        "    print(\"Warning: Model not found. Using text logic only.\")\n",
        "    ner_pipeline = None\n",
        "\n",
        "# --- 2. IMPROVED TABLE EXTRACTION (The Fix) ---\n",
        "def extract_table_robust(text):\n",
        "    rows = []\n",
        "    print(\"   > Scanning text for Balance Sheet items...\")\n",
        "\n",
        "    # REGEX EXPLANATION:\n",
        "    # 1. Look for \"Total assets\" or \"Total liabilities\" (Case insensitive)\n",
        "    # 2. Allow for any junk characters like . or : or spaces\n",
        "    # 3. Allow for an optional $ sign with spaces\n",
        "    # 4. Capture the first number found (e.g., 411,976)\n",
        "\n",
        "    # Pattern: Label -> Any text -> Optional $ -> Number\n",
        "    assets_pattern = r\"Total assets.*?[\\$]?\\s?([\\d,]{3,})\"\n",
        "    liab_pattern = r\"Total liabilities.*?[\\$]?\\s?([\\d,]{3,})\"\n",
        "\n",
        "    # Search\n",
        "    assets = re.search(assets_pattern, text, re.IGNORECASE)\n",
        "    liabilities = re.search(liab_pattern, text, re.IGNORECASE)\n",
        "\n",
        "    if assets:\n",
        "        print(f\"     Found Assets: {assets.group(1)}\")\n",
        "        rows.append({\"item\": \"Total Assets\", \"value\": assets.group(1)})\n",
        "\n",
        "    if liabilities:\n",
        "        print(f\"     Found Liabilities: {liabilities.group(1)}\")\n",
        "        rows.append({\"item\": \"Total Liabilities\", \"value\": liabilities.group(1)})\n",
        "\n",
        "    if rows:\n",
        "        return {\n",
        "            \"section\": \"Financial Statements\",\n",
        "            \"table_type\": \"Balance Sheet\",\n",
        "            \"rows\": rows\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# --- 3. MD&A EXTRACTION (Same as before) ---\n",
        "def extract_mda_exact(text):\n",
        "    results = []\n",
        "    lines = text.split('\\n')\n",
        "    metric_pattern = r'(?:revenue|operating income|net income|profit|sales)'\n",
        "    value_pattern = r'(?:[\\$‚Ç¨‚Çπ‚Ç©]\\s?)?(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)\\s?(?:billion|million)?'\n",
        "\n",
        "    for line in lines:\n",
        "        if re.search(value_pattern, line) and re.search(metric_pattern, line, re.IGNORECASE):\n",
        "            metric = re.search(metric_pattern, line, re.IGNORECASE)\n",
        "            val = re.search(value_pattern, line)\n",
        "\n",
        "            # Filter out \"365\" (Microsoft 365) to be cleaner\n",
        "            if val and val.group(0) == \"365\": continue\n",
        "\n",
        "            results.append({\n",
        "                \"company\": \"Microsoft\",\n",
        "                \"metric\": metric.group(0).lower(),\n",
        "                \"value\": val.group(0),\n",
        "                \"period\": \"2023\",\n",
        "                \"section\": \"MD&A\"\n",
        "            })\n",
        "    return results[:3]\n",
        "\n",
        "# --- 4. EXECUTE ---\n",
        "def process_final(pdf_file):\n",
        "    print(f\"Processing PDF: {pdf_file}...\")\n",
        "    final_output = []\n",
        "\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        # Read enough pages to ensure we hit the Balance Sheet (usually page 40-60 in 10-Ks)\n",
        "        # We'll read the whole text to be safe\n",
        "        full_text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            full_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        # A. MD&A\n",
        "        mda_data = extract_mda_exact(full_text)\n",
        "        final_output.extend(mda_data)\n",
        "\n",
        "        # B. Table\n",
        "        table_data = extract_table_robust(full_text)\n",
        "        if table_data:\n",
        "            final_output.append(table_data)\n",
        "        else:\n",
        "            final_output.append({\"section\": \"Financial Statements\", \"status\": \"Table not found\"})\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Run\n",
        "final_json = process_final(pdf_path)\n",
        "\n",
        "print(\"\\n=============== FINAL CORRECT JSON ===============\\n\")\n",
        "print(json.dumps(final_json, indent=4))\n",
        "\n",
        "# Save\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(final_json, f, indent=4)"
      ],
      "metadata": {
        "id": "1zO3ZLo99XEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: INSTALL PDF GENERATOR ---\n",
        "!pip install reportlab\n",
        "\n",
        "import json\n",
        "import os\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "# --- STEP 2: DEFINE PATHS ---\n",
        "json_path = '/content/drive/MyDrive/dataset/final_submission.json'\n",
        "pdf_output_path = '/content/drive/MyDrive/dataset/AI_Financial_Report.pdf'\n",
        "\n",
        "# --- STEP 3: GENERATE PDF ---\n",
        "def create_final_pdf(json_file, pdf_file):\n",
        "    print(\"Reading JSON data...\")\n",
        "    if not os.path.exists(json_file):\n",
        "        print(\"‚ùå Error: JSON file not found. Run the extraction code first!\")\n",
        "        return\n",
        "\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(\"Building PDF Report...\")\n",
        "    doc = SimpleDocTemplate(pdf_file, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = []\n",
        "\n",
        "    # Title\n",
        "    story.append(Paragraph(\"AI Financial Analysis Report\", styles['Title']))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph(\"<b>Generated by:</b> AI Financial Analyst (Milestone 4)\", styles['Normal']))\n",
        "    story.append(Spacer(1, 24))\n",
        "\n",
        "    # Section 1: MD&A\n",
        "    story.append(Paragraph(\"1. Management's Discussion & Analysis (MD&A)\", styles['Heading2']))\n",
        "    story.append(Spacer(1, 10))\n",
        "\n",
        "    mda_data = [d for d in data if d.get('section') == 'MD&A']\n",
        "    if mda_data:\n",
        "        for item in mda_data:\n",
        "            text = f\"The AI detected that <b>{item.get('company')}</b> reported a <b>{item.get('metric')}</b> of <b>{item.get('value')}</b> for {item.get('period')}.\"\n",
        "            story.append(Paragraph(f\"‚Ä¢ {text}\", styles['Normal']))\n",
        "            story.append(Spacer(1, 5))\n",
        "    else:\n",
        "        story.append(Paragraph(\"No MD&A metrics extracted.\", styles['Normal']))\n",
        "\n",
        "    story.append(Spacer(1, 20))\n",
        "\n",
        "    # Section 2: Balance Sheet\n",
        "    story.append(Paragraph(\"2. Financial Statements (Balance Sheet)\", styles['Heading2']))\n",
        "    story.append(Spacer(1, 10))\n",
        "\n",
        "    table_data = [d for d in data if d.get('section') == 'Financial Statements']\n",
        "\n",
        "    if table_data and 'rows' in table_data[0]:\n",
        "        # Create Table Header and Rows\n",
        "        t_data = [['Line Item', 'Value']] # Header\n",
        "        for row in table_data[0]['rows']:\n",
        "            t_data.append([row['item'], row['value']])\n",
        "\n",
        "        # Style Table\n",
        "        t = Table(t_data, colWidths=[300, 150])\n",
        "        t.setStyle(TableStyle([\n",
        "            ('BACKGROUND', (0,0), (-1,0), colors.darkblue),\n",
        "            ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n",
        "            ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
        "            ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
        "            ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
        "            ('GRID', (0,0), (-1,-1), 1, colors.black)\n",
        "        ]))\n",
        "        story.append(t)\n",
        "    else:\n",
        "        story.append(Paragraph(\"No Balance Sheet table found.\", styles['Normal']))\n",
        "\n",
        "    # Save\n",
        "    doc.build(story)\n",
        "    print(f\"‚úÖ PDF Generated Successfully!\")\n",
        "    print(f\"üìÇ Location: {pdf_file}\")\n",
        "\n",
        "# --- EXECUTE ---\n",
        "create_final_pdf(json_path, pdf_output_path)"
      ],
      "metadata": {
        "id": "wSWe1SecL2Mn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}